<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[^memory^blog]]></title>
  <link href="http://jwoffindin.github.io/atom.xml" rel="self"/>
  <link href="http://jwoffindin.github.io/"/>
  <updated>2013-08-29T17:18:09+12:00</updated>
  <id>http://jwoffindin.github.io/</id>
  <author>
    <name><![CDATA[John Woffindin]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Migrating a AWS VPC between regions - part 1]]></title>
    <link href="http://jwoffindin.github.io/blog/2013/08/20/migrating-aws-images-between-regions/"/>
    <updated>2013-08-20T10:10:00+12:00</updated>
    <id>http://jwoffindin.github.io/blog/2013/08/20/migrating-aws-images-between-regions</id>
    <content type="html"><![CDATA[<p><strong>Diclaimer: This post is a work in progress. It&rsquo;s disorganised and any
commands presented below are likey to destroy your production database,
so take the following with a large grain of salt</strong></p>

<p>This is first part of a series that describes the process I went through
migrating AWS-based VPC instances from US region to AU and using
CloudFormation for setting up VPC. It is also an excuse to finally start
a blog and kick the tires of <a href="http://octopress.org/">Octopress</a>.</p>

<h2>Background</h2>

<p>We have two VPC networks on AWS with servers split across several
subnets. OS is Oracle Enterprise Linux 5.5. The original AMIs are no
longer provided by Oracle, and there are no corresponding Kernel (AKI)
or Ramdisk Image (ARI) in the <code>ap-southeast-2</code> region, so using AWS EC2
Copy [does not work][1].</p>

<p>Fortunately the target region supports <a href="http://wiki.xen.org/wiki/PvGrub">pv-grub</a> so the migration will
will also include making migrated server bootable via pv-grub.</p>

<p>So, the plan is to:</p>

<ol>
<li>Write script to copy root and data EBS volumes to the &ldquo;target&rdquo;
region.</li>
<li>Update these copied root volumes to make then bootable under pv-grub
the target region. Each server has an unknown set of changes made so
we must migrate all instances.</li>
<li>Create a CloudFormation template referencing migrated AMIs so
instances are created in the correct subnets and correct data volumes
are mounted on the correct hosts.</li>
<li>Create the server instances using CloudFormation and test</li>
<li>Clean up after ourselves.</li>
</ol>


<p>The first step is straight forward so I&rsquo;ll cover it here. The remaining
activities will be covered in their own posts.</p>

<h2>Migrating Root Images</h2>

<p>Each of the instances has two EBS volumes &ndash; a <em>system</em> EBS volume
(containing the root and boot partitions) and a <em>data</em> volume. The
system volume is a disk image with 3 parititons:</p>

<ul>
<li><code>/dev/sda1</code> &ndash; the <code>/boot</code> volume</li>
<li><code>/dev/sda1</code> &ndash; the root volume (mounted as &lsquo;/&rsquo;)</li>
<li><code>/dev/sda3</code> &ndash; swap (more on this one later)</li>
</ul>


<p>Below is a simple ZSH script I wrote to migrate root EBS
volumes from one region to another. It sputs out a CSV file
which we&rsquo;ll need later to correlate the migrated instances against the
original instance ids.</p>

<div><script src='https://gist.github.com/6373731.js'></script>
<noscript><pre><code></code></pre></noscript></div>


<p>The current version only migrates the root images, will update later
with something a bit smarter.</p>

<p>Note that snapshotting the filesystems is a bit tricky since the OREL 5.5
systems aren&rsquo;t using LVM and don&rsquo;t support [fsfreeze].
As a workaround, the script will remount the root filesystem read-only,
trigger ec2 snapshot and then remount as writeable. The issue is that we
are not handling application consistency correctly (e.g. databases).</p>
]]></content>
  </entry>
  
</feed>
